# -*- coding: utf-8 -*-
"""Brigidi Blay_SportsPrediction 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dMAtGWTgvdGXAYed-bwRJ1jU9bXFQ0nN
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
import xgboost as xgb
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV
import pickle
from sklearn.linear_model import LinearRegression
from xgboost import XGBRegressor
from scipy.stats import randint, uniform

male_players_file_path = '/content/drive/My Drive/male_players.csv'

male_players_data = pd.read_csv(male_players_file_path)

male_players_data.head()

"""# Data Preprocessing"""

# Function To Drop Columns With A Low Correlation

def drop_low_correlation_columns(overall_dataframe, target_column, threshold=0.5):
    numerical_data = overall_dataframe.select_dtypes(include=['number'])
    columns_to_drop = []

    for column in numerical_data.columns:
        if column != target_column:
            corr = numerical_data[column].corr(numerical_data[target_column])
            if abs(corr) < threshold or pd.isna(corr):
                columns_to_drop.append(column)

    overall_dataframe = overall_dataframe.drop(columns=columns_to_drop)
    return overall_dataframe

result = drop_low_correlation_columns(male_players_data, 'overall')
result.head()

# Function to perform one hot encoding on the preferred foot column

def one_hot_encode_preferred_foot(df):
    result = df.copy()
    result = pd.get_dummies(result, columns=['preferred_foot'], prefix='foot')
    foot_columns = [col for col in result.columns if col.startswith('foot_')]
    for col in foot_columns:
        result[col] = result[col].astype(int)
    return result

result = one_hot_encode_preferred_foot(result)

result.head()

# Function to Drop Columns With More than 30 Percent Null Values

def drop_high_null_columns(data, threshold=0.3):
    thresh_count = int((1 - threshold) * len(data))
    data_cleaned = data.dropna(axis=1, thresh=thresh_count)
    return data_cleaned

result = drop_high_null_columns(result, threshold=0.3)

result.head()

# Dropping all categorical columns
def drop_categorical_columns(data):
    categorical_columns = data.select_dtypes(include=['object']).columns
    data = data.drop(columns=categorical_columns)
    return data

result = drop_categorical_columns(result)

"""Imputing"""

# Function to insert the median value where there are Null values
def median_imputing(dataframes):
  for col in dataframes.columns:
    column_median = dataframes[col].median(skipna=True)
    dataframes[col] = dataframes[col].fillna(column_median)
  return dataframes

result = median_imputing(result)

result.info()

"""# Feature Engineering"""

# I have already dropped the columns with low correlation, so it is just left with scaling

from sklearn.preprocessing import StandardScaler

def scale_independent_variables(dataframe, target_column):
    df_scaled = dataframe.copy()
    columns_to_scale = [col for col in df_scaled.columns if col != target_column]
    scaler = StandardScaler()
    df_scaled[columns_to_scale] = scaler.fit_transform(df_scaled[columns_to_scale])

    return df_scaled, scaler

scaled_df, scaler = scale_independent_variables(result, 'overall')
result = scaled_df

"""#Training The model"""

# Separating our features from our labels
X = result.drop("overall",axis=1) # Features
y = result["overall"] # Labels

# Splitting our data into training and testing modules
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)
X_train.head()

"""# Training 3 Ensemble Models"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

models = {
    "RandomForest": RandomForestRegressor(n_estimators=100, random_state=42),
    "XGBoost": xgb.XGBRegressor(n_estimators=100, random_state=42),
    "GradientBoost": GradientBoostingRegressor(n_estimators=100, random_state=42)
}
for model_name, model in models.items():
    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
    print(f"{model_name} Cross-Validation R2 Scores: {scores}")
    print(f"{model_name} Mean Cross-Validation R2 Score: {np.mean(scores)}")
    model.fit(X_train, y_train)
    test_score = model.score(X_test, y_test)
    print(f"{model_name} Test R2 Score: {test_score}\n")

"""# Training 3 Basic Models"""

# Linear Regression

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
Linear_model = LinearRegression()
Linear_model.fit(X_train, y_train)
y_pred_test = Linear_model.predict(X_test)
Linear_test_r2 = r2_score(y_test, y_pred_test)
Linear_test_mae = mean_absolute_error(y_test, y_pred_test)

# Decision Tree Regression
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
Decision_Tree_model = DecisionTreeRegressor(random_state=42)
Decision_Tree_model.fit(X_train, y_train)
Decision_Tree_y_pred_test = Decision_Tree_model.predict(X_test)
Decision_Tree_test_r2 = r2_score(y_test, Decision_Tree_y_pred_test)
Decision_Tree_test_mae = mean_absolute_error(y_test, Decision_Tree_y_pred_test)

"""# Evaluation and Hypertuning"""

# Using evaluation metrics to assess our basic models

#Decision Tree Regression
test_mae = mean_absolute_error(y_test, y_pred_test)
print("Decision Tree Regression without hyperparameters:")
print(f"Test R2 score: {Decision_Tree_test_r2}")
print(f"Test MAE: {Decision_Tree_test_mae}")

# Evaluation On Linear Regression

# Linear Regression
print("Linear Regression without hyperparameters:")
print(f"Test R2 score: {Linear_test_r2}")
print(f"Test MAE: {Linear_test_mae}")

# Using evaluation metrics to assess our ensemble models
for model_name, model in models.items():
    model.fit(X_train, y_train)

    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)

    test_r2 = model.score(X_test, y_test)
    train_mae = mean_absolute_error(y_train, y_pred_train)
    test_mae = mean_absolute_error(y_test, y_pred_test)

    print(f"{model_name} Train MAE: {train_mae}")
    print(f"{model_name} Test MAE: {test_mae}")
    print(f"{model_name} Test R2 Score: {test_r2}\n")

# Hypertuning My Ensemble Model Of Choice

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
xgb_model = models["XGBoost"]

param_dist = {
    'n_estimators': randint(100, 500),
    'max_depth': randint(3, 10),
    'learning_rate': uniform(0.01, 0.3),
}

random_search = RandomizedSearchCV(
    xgb_model,
    param_distributions=param_dist,
    n_iter=12,
    cv=5,
    scoring='r2',
    n_jobs=-1,
    random_state=42
)

random_search.fit(X_train, y_train)

print(f"Best parameters for XGBoost: {random_search.best_params_}")
print(f"Best cross-validation R2 score for XGBoost: {random_search.best_score_}")

best_model = random_search.best_estimator_

best_model.fit(X_train, y_train)
y_pred_train = best_model.predict(X_train)
y_pred_test = best_model.predict(X_test)

train_r2 = r2_score(y_train, y_pred_train)
test_r2 = r2_score(y_test, y_pred_test)
train_mae = mean_absolute_error(y_train, y_pred_train)
test_mae = mean_absolute_error(y_test, y_pred_test)

# Evaluating my hypertuned model

print(f"XGBoost Train R2 Score: {train_r2}")
print(f"XGBoost Test R2 Score: {test_r2}")
print(f"XGBoost Train MAE: {train_mae}")
print(f"XGBoost Test MAE: {test_mae}")

"""# Loading and Preparing My Test Dataset"""

players_data_path = '/content/drive/My Drive/players_22.csv'
players_data = pd.read_csv(players_data_path)
players_data.head()

# Dropping Columns with Low Correlation in The Testing Dataset
players_data = drop_low_correlation_columns(players_data, 'overall')

# Performing One Hot Encoding On The Preferred Foot Column
players_data = one_hot_encode_preferred_foot(players_data)

# Dropping The Columns With Null Values greater than 30 percent of the total column
players_data = drop_high_null_columns(players_data, threshold=0.3)

# Dropping All Categorical Columns From My DataSet
players_data = drop_categorical_columns(players_data)

# Using median imputing to fill the null results with the median of the columns
players_data = median_imputing(players_data)

# Scaling the independent variables in my testing data set
scaled_df, scaler = scale_independent_variables(players_data, 'overall')
players_data = scaled_df

players_data

# Making sure that only the columns available in the training data set are available in the testing data set
common_columns = result.columns
filtered_players_data = players_data[common_columns]

# Testing my model against unseen data

X_test.info()

# Separating features from labels
X_test, y_test = filtered_players_data, filtered_players_data['overall']
X_test = X_test.drop('overall', axis=1)

# Make predictions
y_pred = best_model.predict(X_test)

# Evaluate the model's performance
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"Mean Absolute Error: {mae}")
print(f"Mean Squared Error: {mse}")
print(f"Root Mean Squared Error: {rmse}")
print(f"R-squared Score: {r2}")

# Using Feature Importance To Determine which Features To Put On My Webapp

feature_importances = best_model.feature_importances_

feature_importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': feature_importances
})

feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 8))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')
plt.gca().invert_yaxis()
plt.show()

top_features = feature_importance_df.head(10)['Feature']
print("Top Features:")
print(top_features)





# Downloading Scalar Used On Training and Test Data To Be Applied Onto User's Input
with open('scalernew.pkl', 'wb') as file:
    pickle.dump(scaler, file)

with open('best_model.pkl', 'wb') as file:
    pickle.dump(best_model, file)

